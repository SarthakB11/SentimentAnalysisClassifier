{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download and Extract Cornell Movie Review Polarity Dataset"
      ],
      "metadata": {
        "id": "EVLA2VnmyGoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "# URL of the dataset\n",
        "url = \"https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\"\n",
        "dataset_folder = \"rt-polaritydata\"\n",
        "\n",
        "# Function to download the dataset\n",
        "def download_dataset(url, download_path):\n",
        "    if not os.path.exists(download_path):\n",
        "        print(f\"Downloading dataset from {url}...\")\n",
        "        urllib.request.urlretrieve(url, download_path)\n",
        "        print(\"Download complete.\")\n",
        "    else:\n",
        "        print(\"Dataset already downloaded.\")\n",
        "\n",
        "# Function to extract the tar.gz file\n",
        "def extract_dataset(tar_path, extract_to):\n",
        "    if not os.path.exists(extract_to):\n",
        "        print(f\"Extracting {tar_path}...\")\n",
        "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "            tar.extractall(path=extract_to)\n",
        "        print(f\"Extraction complete. Files extracted to {extract_to}\")\n",
        "    else:\n",
        "        print(\"Dataset already extracted.\")\n",
        "\n",
        "# Download and extract dataset\n",
        "download_path = \"rt-polaritydata.tar.gz\"\n",
        "download_dataset(url, download_path)\n",
        "extract_dataset(download_path, dataset_folder)\n",
        "\n",
        "# List the extracted files\n",
        "print(f\"Extracted files: {os.listdir(dataset_folder)}\")\n"
      ],
      "metadata": {
        "id": "uy2tDn_oC-6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09af72cc-a230-400e-d58e-09b339e8dd97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset from https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz...\n",
            "Download complete.\n",
            "Extracting rt-polaritydata.tar.gz...\n",
            "Extraction complete. Files extracted to rt-polaritydata\n",
            "Extracted files: ['rt-polaritydata.README.1.0.txt', 'rt-polaritydata']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd rt-polaritydata/rt-polaritydata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6rKfGhVE9R2",
        "outputId": "fece3c91-3408-4a6c-e31d-ac7c02c1e427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/rt-polaritydata/rt-polaritydata\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88KOTniuFHQt",
        "outputId": "b39991fd-9627-4546-9675-3a3b2ef65e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rt-polarity.neg  rt-polarity.pos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performing TF-IDF"
      ],
      "metadata": {
        "id": "bq-lCGW8yQ2x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBjf6pTQAsCI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load data\n",
        "pos_reviews = open(\"rt-polarity.pos\", \"r\", encoding=\"latin-1\").readlines()\n",
        "neg_reviews = open(\"rt-polarity.neg\", \"r\", encoding=\"latin-1\").readlines()\n",
        "\n",
        "# Create labels\n",
        "pos_labels = [1] * len(pos_reviews)\n",
        "neg_labels = [0] * len(neg_reviews)\n",
        "\n",
        "# Combine data\n",
        "all_reviews = pos_reviews + neg_reviews\n",
        "all_labels = pos_labels + neg_labels\n",
        "\n",
        "# Train-validation-test split\n",
        "train_texts = all_reviews[:8000]\n",
        "train_labels = all_labels[:8000]\n",
        "\n",
        "val_texts = all_reviews[8000:9000]\n",
        "val_labels = all_labels[8000:9000]\n",
        "\n",
        "test_texts = all_reviews[9000:]\n",
        "test_labels = all_labels[9000:]\n",
        "\n",
        "# Text Vectorization (TF-IDF)\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(train_texts)\n",
        "X_val = vectorizer.transform(val_texts)\n",
        "X_test = vectorizer.transform(test_texts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Without preprocessing, Perform Logistic Regression"
      ],
      "metadata": {
        "id": "IKeynGbkFW53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, train_labels)\n",
        "\n",
        "# Validation\n",
        "val_preds = clf.predict(X_val)\n",
        "print(f\"Validation Accuracy: {accuracy_score(val_labels, val_preds)}\")\n",
        "\n",
        "# Testing\n",
        "test_preds = clf.predict(X_test)\n",
        "print(f\"Test Accuracy: {accuracy_score(test_labels, test_preds)}\")\n",
        "print(classification_report(test_labels, test_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4f2IifBFVyc",
        "outputId": "69b345fb-3f39-4d29-83ee-f680a1d6a0fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.417\n",
            "Test Accuracy: 0.4362214199759326\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.44      0.61      1662\n",
            "           1       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.44      1662\n",
            "   macro avg       0.50      0.22      0.30      1662\n",
            "weighted avg       1.00      0.44      0.61      1662\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM Classifier"
      ],
      "metadata": {
        "id": "CSPjV1T6ya4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================\n",
        "# SVM Classifier\n",
        "# ==================\n",
        "from sklearn import svm\n",
        "print(\"\\nTraining SVM Classifier...\")\n",
        "svm_clf = svm.SVC(kernel='linear', C=1.0)\n",
        "svm_clf.fit(X_train, train_labels)\n",
        "\n",
        "# Validation accuracy for SVM\n",
        "val_preds_svm = svm_clf.predict(X_val)\n",
        "print(f\"SVM Validation Accuracy: {accuracy_score(val_labels, val_preds_svm)}\")\n",
        "\n",
        "# Test accuracy for SVM\n",
        "test_preds_svm = svm_clf.predict(X_test)\n",
        "print(f\"SVM Test Accuracy: {accuracy_score(test_labels, test_preds_svm)}\")\n",
        "print(\"SVM Classification Report:\")\n",
        "print(classification_report(test_labels, test_preds_svm))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8btEJfTTFeNA",
        "outputId": "f23a75df-37ec-411b-d72b-f897435a2c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training SVM Classifier...\n",
            "SVM Validation Accuracy: 0.546\n",
            "SVM Test Accuracy: 0.5511432009626955\n",
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.55      0.71      1662\n",
            "           1       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.55      1662\n",
            "   macro avg       0.50      0.28      0.36      1662\n",
            "weighted avg       1.00      0.55      0.71      1662\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Classifier"
      ],
      "metadata": {
        "id": "IakHo2ZRyfo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==================\n",
        "# Random Forest Classifier\n",
        "# ==================\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "print(\"\\nTraining Random Forest Classifier...\")\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, train_labels)\n",
        "\n",
        "# Validation accuracy for Random Forest\n",
        "val_preds_rf = rf_clf.predict(X_val)\n",
        "print(f\"Random Forest Validation Accuracy: {accuracy_score(val_labels, val_preds_rf)}\")\n",
        "\n",
        "# Test accuracy for Random Forest\n",
        "test_preds_rf = rf_clf.predict(X_test)\n",
        "print(f\"Random Forest Test Accuracy: {accuracy_score(test_labels, test_preds_rf)}\")\n",
        "print(\"Random Forest Classification Report:\")\n",
        "print(classification_report(test_labels, test_preds_rf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t463xDunFzRr",
        "outputId": "646b0747-3430-43b1-b2b9-a111c58bd01c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Random Forest Classifier...\n",
            "Random Forest Validation Accuracy: 0.291\n",
            "Random Forest Test Accuracy: 0.31167268351383876\n",
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.31      0.48      1662\n",
            "           1       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.31      1662\n",
            "   macro avg       0.50      0.16      0.24      1662\n",
            "weighted avg       1.00      0.31      0.48      1662\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With preprocessing\n",
        "\n",
        "*   Vectorization through TF-IDF\n",
        "*   Running for Logistic Regression, Support Vector Machine, Random Forest Classifier\n",
        "\n"
      ],
      "metadata": {
        "id": "FmEK0DtJy6wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n",
        "\n",
        "# Download required nltk data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize lemmatizer and stop words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "default_stopwords = set(stopwords.words('english'))\n",
        "custom_stopwords = default_stopwords - {'not', 'no', 'nor'}\n",
        "\n",
        "# Preprocessing functions\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "def remove_special_chars(text):\n",
        "    return re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = word_tokenize(text)\n",
        "    return \" \".join([word for word in words if word not in custom_stopwords])\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    words = word_tokenize(text)\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = to_lowercase(text)\n",
        "    text = remove_special_chars(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = lemmatize_text(text)\n",
        "    return text\n",
        "\n",
        "# Load data\n",
        "pos_reviews = open(\"rt-polarity.pos\", \"r\", encoding=\"latin-1\").readlines()\n",
        "neg_reviews = open(\"rt-polarity.neg\", \"r\", encoding=\"latin-1\").readlines()\n",
        "\n",
        "# Create labels\n",
        "pos_labels = [1] * len(pos_reviews)\n",
        "neg_labels = [0] * len(neg_reviews)\n",
        "\n",
        "# Combine data\n",
        "all_reviews = pos_reviews + neg_reviews\n",
        "all_labels = pos_labels + neg_labels\n",
        "\n",
        "# Preprocess all reviews\n",
        "all_reviews = [preprocess_text(review) for review in all_reviews]\n",
        "\n",
        "# Train-validation-test split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(all_reviews, all_labels, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Text Vectorization (TF-IDF)\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Model Training and Evaluation\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Support Vector Machine\": SVC(),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    # Fit the model\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "    results[model_name] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": report['weighted avg']['precision'],\n",
        "        \"recall\": report['weighted avg']['recall'],\n",
        "        \"f1-score\": report['weighted avg']['f1-score']\n",
        "    }\n",
        "\n",
        "# Convert results to DataFrame for better visualization\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox8iE4clIElu",
        "outputId": "6727cda4-e238-48a6-aa93-e157be3a959e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        accuracy  precision   recall  f1-score\n",
            "Logistic Regression      0.75750   0.757514  0.75750  0.757497\n",
            "Support Vector Machine   0.76125   0.761276  0.76125  0.761244\n",
            "Random Forest            0.68875   0.689490  0.68875  0.688446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making Custom Model\n",
        "\n",
        "*   Using Glove Embeddings to prepare Embedding Matrix\n",
        "*   Sequential + Embedding + LSTM\n",
        "\n"
      ],
      "metadata": {
        "id": "hcjpiqD0zccx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ-MXTz6i_u9",
        "outputId": "327ea9dd-e232-4cef-dd1e-a628cb9b77ce",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-25 19:03:37--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-09-25 19:03:37--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-09-25 19:03:38--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.18MB/s    in 2m 40s  \n",
            "\n",
            "2024-09-25 19:06:18 (5.15 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYbq3hMhlKD9",
        "outputId": "2753e24a-1cf2-4862-84b6-36834d58d746",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip     rt-polarity.pos\n",
            "glove.6B.200d.txt  glove.6B.50d.txt   rt-polarity.neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install Keras-Preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGJHJ5b3lUz7",
        "outputId": "8813c366-f8d1-4e83-ce6c-2bb3e6019680",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Collecting Keras-Preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.26.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.16.0)\n",
            "Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Keras-Preprocessing\n",
            "Successfully installed Keras-Preprocessing-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dropout, LSTM\n",
        "import nltk\n",
        "\n",
        "# Download required nltk data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize lemmatizer and stop words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "default_stopwords = set(stopwords.words('english'))\n",
        "custom_stopwords = default_stopwords - {'not', 'no', 'nor'}\n",
        "\n",
        "# Preprocessing functions\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "def remove_special_chars(text):\n",
        "    return re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = word_tokenize(text)\n",
        "    return \" \".join([word for word in words if word not in custom_stopwords])\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    words = word_tokenize(text)\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = to_lowercase(text)\n",
        "    text = remove_special_chars(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = lemmatize_text(text)\n",
        "    return text\n",
        "\n",
        "# Load data\n",
        "pos_reviews = open(\"rt-polarity.pos\", \"r\", encoding=\"latin-1\").readlines()\n",
        "neg_reviews = open(\"rt-polarity.neg\", \"r\", encoding=\"latin-1\").readlines()\n",
        "\n",
        "# Create labels\n",
        "pos_labels = [1] * len(pos_reviews)\n",
        "neg_labels = [0] * len(neg_reviews)\n",
        "\n",
        "# Combine data\n",
        "all_reviews = pos_reviews + neg_reviews\n",
        "all_labels = pos_labels + neg_labels\n",
        "\n",
        "# Preprocess all reviews\n",
        "all_reviews = [preprocess_text(review) for review in all_reviews]\n",
        "\n",
        "# Train-validation-test split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(all_reviews, all_labels, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Load GloVe embeddings\n",
        "def load_glove_embeddings(file_path):\n",
        "    embeddings_index = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    return embeddings_index\n",
        "\n",
        "# Specify GloVe file path (download and use 'glove.6B.100d.txt' or any other variant)\n",
        "glove_file_path = 'glove.6B.300d.txt'  # Replace with the correct path to your GloVe file\n",
        "embeddings_index = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "# Prepare the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences\n",
        "max_length = max(len(x) for x in X_train_seq)  # Get the maximum length of the sequences\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length)\n",
        "X_val_pad = pad_sequences(X_val_seq, maxlen=max_length)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length)\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_dim = 300\n",
        "word_index = tokenizer.word_index\n",
        "num_words = min(20000, len(word_index) + 1)  # Limit to 20,000 words\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i < num_words:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Build the optimized model\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "model.add(LSTM(128, return_sequences=False))  # LSTM layer\n",
        "model.add(Dropout(0.5))  # Dropout layer\n",
        "model.add(Dense(64, activation='relu'))  # Increased complexity\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model with a different learning rate if necessary\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "X_train_pad = np.array(X_train_pad)\n",
        "y_train = np.array(y_train)\n",
        "X_val_pad = np.array(X_val_pad)\n",
        "y_val = np.array(y_val)\n",
        "model.fit(X_train_pad, y_train, epochs=10, batch_size=32, validation_data=(X_val_pad, y_val), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "X_test_pad = np.array(X_test_pad)\n",
        "y_test = np.array(y_test)\n",
        "loss, accuracy = model.evaluate(X_test_pad, y_test)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QFyi7TplDOy",
        "outputId": "14b11370-f251-4990-f105-8dbca3010e21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 101ms/step - accuracy: 0.6680 - loss: 0.6017 - val_accuracy: 0.7480 - val_loss: 0.5111\n",
            "Epoch 2/10\n",
            "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 103ms/step - accuracy: 0.7700 - loss: 0.4779 - val_accuracy: 0.7630 - val_loss: 0.4844\n",
            "Epoch 3/10\n",
            "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 108ms/step - accuracy: 0.7836 - loss: 0.4514 - val_accuracy: 0.7686 - val_loss: 0.4872\n",
            "Epoch 4/10\n",
            "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 103ms/step - accuracy: 0.8299 - loss: 0.3747 - val_accuracy: 0.7598 - val_loss: 0.4811\n",
            "Epoch 5/10\n",
            "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 110ms/step - accuracy: 0.8623 - loss: 0.3221 - val_accuracy: 0.7649 - val_loss: 0.5267\n",
            "Epoch 6/10\n",
            "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 101ms/step - accuracy: 0.8955 - loss: 0.2552 - val_accuracy: 0.7636 - val_loss: 0.5418\n",
            "Epoch 7/10\n",
            "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 95ms/step - accuracy: 0.9224 - loss: 0.1964 - val_accuracy: 0.7517 - val_loss: 0.6788\n",
            "Epoch 8/10\n",
            "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 100ms/step - accuracy: 0.9376 - loss: 0.1493 - val_accuracy: 0.7555 - val_loss: 0.7305\n",
            "Epoch 9/10\n",
            "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 100ms/step - accuracy: 0.9608 - loss: 0.1045 - val_accuracy: 0.7480 - val_loss: 0.8582\n",
            "Epoch 10/10\n",
            "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 103ms/step - accuracy: 0.9652 - loss: 0.0919 - val_accuracy: 0.7523 - val_loss: 0.9067\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.7453 - loss: 1.0165\n",
            "Test Accuracy: 0.7444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Bert Tokenizer"
      ],
      "metadata": {
        "id": "8sx4q_oZ0U7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-hub\n",
        "!pip install transformers torch\n",
        "!pip install torch-xla"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYwOFrx6mtCY",
        "outputId": "c97388c4-b7ac-4cde-d23a-a3b401232633",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (3.20.3)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (2.15.1)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tf-keras>=2.14.1->tensorflow-hub) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (24.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.66.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torch-xla in /usr/local/lib/python3.10/dist-packages (2.4.0+libtpu)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from torch-xla) (1.4.0)\n",
            "Requirement already satisfied: cloud-tpu-client>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from torch-xla) (0.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from torch-xla) (6.0.2)\n",
            "Requirement already satisfied: google-api-python-client==1.8.0 in /usr/local/lib/python3.10/dist-packages (from cloud-tpu-client>=0.10.0->torch-xla) (1.8.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from cloud-tpu-client>=0.10.0->torch-xla) (4.1.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (0.22.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (0.2.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (1.34.1)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.0.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla) (0.4.1)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla) (4.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (5.5.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Download required nltk data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize stop words\n",
        "default_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# Preprocessing functions\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters\n",
        "    words = nltk.word_tokenize(text)  # Tokenize text\n",
        "    words = [word for word in words if word not in default_stopwords]  # Remove stopwords\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Load data\n",
        "pos_reviews = open(\"rt-polarity.pos\", \"r\", encoding=\"latin-1\").readlines()\n",
        "neg_reviews = open(\"rt-polarity.neg\", \"r\", encoding=\"latin-1\").readlines()\n",
        "\n",
        "# Create labels\n",
        "pos_labels = [1] * len(pos_reviews)\n",
        "neg_labels = [0] * len(neg_reviews)\n",
        "\n",
        "# Combine data\n",
        "all_reviews = pos_reviews + neg_reviews\n",
        "all_labels = pos_labels + neg_labels\n",
        "\n",
        "# Preprocess all reviews\n",
        "all_reviews = [preprocess_text(review) for review in all_reviews]\n",
        "\n",
        "# Train-validation-test split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(all_reviews, all_labels, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Create a custom dataset class\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = tokenizer.encode_plus(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=128,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create datasets and DataLoaders\n",
        "train_dataset = SentimentDataset(X_train, y_train)\n",
        "val_dataset = SentimentDataset(X_val, y_val)\n",
        "test_dataset = SentimentDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Create a simple classifier using BERT\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, 2)  # Binary classification\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs[1]  # Get pooled output for classification\n",
        "        output = self.dropout(pooled_output)\n",
        "        return self.fc(output)\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "classifier = SentimentClassifier()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training Loop\n",
        "def train_model(model, data_loader):\n",
        "    model.train()\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['label']\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Validation Loop\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['label']\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            predictions.extend(preds.numpy())\n",
        "            true_labels.extend(labels.numpy())\n",
        "    return accuracy_score(true_labels, predictions)\n",
        "\n",
        "# Train and evaluate\n",
        "for epoch in range(5):  # Fewer epochs to speed up the process\n",
        "    train_model(classifier, train_loader)\n",
        "    val_accuracy = evaluate_model(classifier, val_loader)\n",
        "    print(f'Epoch {epoch + 1}, Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "# Test the model\n",
        "test_accuracy = evaluate_model(classifier, test_loader)\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZOCR-uTshOe",
        "outputId": "c6510ac7-111e-4fc4-e05d-8f337839676d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Accuracy: 0.7980\n",
            "Epoch 2, Validation Accuracy: 0.7799\n",
            "Epoch 3, Validation Accuracy: 0.8080\n",
            "Epoch 4, Validation Accuracy: 0.7980\n",
            "Epoch 5, Validation Accuracy: 0.8155\n",
            "Test Accuracy: 0.7981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking preprocessing techniques"
      ],
      "metadata": {
        "id": "05ngGDFk0rkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "\n",
        "# Download required nltk data\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize preprocessing tools\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Original dataset (for comparison)\n",
        "original_data = all_reviews[:5]  # Sample 5 reviews for demonstration\n",
        "\n",
        "# Lowercase text\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Remove special characters, punctuation, and numbers\n",
        "def remove_special_chars(text):\n",
        "    return re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "\n",
        "# Remove stop words\n",
        "def remove_stopwords(text):\n",
        "    words = word_tokenize(text)\n",
        "    return \" \".join([word for word in words if word not in stop_words])\n",
        "\n",
        "# Apply lemmatization\n",
        "def lemmatize_text(text):\n",
        "    words = word_tokenize(text)\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "# Apply stemming\n",
        "def stem_text(text):\n",
        "    words = word_tokenize(text)\n",
        "    return \" \".join([stemmer.stem(word) for word in words])\n",
        "\n",
        "# Handle repeated characters (e.g., \"woooow\" -> \"wow\")\n",
        "def remove_repeated_chars(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
        "\n",
        "# Remove short words\n",
        "def remove_short_words(text, min_length=3):\n",
        "    words = word_tokenize(text)\n",
        "    return \" \".join([word for word in words if len(word) >= min_length])\n",
        "\n",
        "# Remove extra whitespaces\n",
        "def remove_extra_whitespaces(text):\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "# Pipeline of preprocessing techniques\n",
        "def preprocess_text(text, lowercase=True, special_chars=True, stopwords=True,\n",
        "                    lemmatize=False, stem=False, repeated_chars=False,\n",
        "                    short_words=False, whitespaces=True):\n",
        "\n",
        "    if lowercase:\n",
        "        text = to_lowercase(text)\n",
        "\n",
        "    if special_chars:\n",
        "        text = remove_special_chars(text)\n",
        "\n",
        "    if stopwords:\n",
        "        text = remove_stopwords(text)\n",
        "\n",
        "    if lemmatize:\n",
        "        text = lemmatize_text(text)\n",
        "\n",
        "    if stem:\n",
        "        text = stem_text(text)\n",
        "\n",
        "    if repeated_chars:\n",
        "        text = remove_repeated_chars(text)\n",
        "\n",
        "    if short_words:\n",
        "        text = remove_short_words(text)\n",
        "\n",
        "    if whitespaces:\n",
        "        text = remove_extra_whitespaces(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Compare different preprocessing techniques\n",
        "techniques = {\n",
        "    \"Original Data\": original_data,\n",
        "    \"Lowercase Only\": [preprocess_text(text, special_chars=False, stopwords=False) for text in original_data],\n",
        "    \"Remove Special Chars\": [preprocess_text(text, stopwords=False) for text in original_data],\n",
        "    \"Remove Stop Words\": [preprocess_text(text) for text in original_data],\n",
        "    \"Lemmatization\": [preprocess_text(text, lemmatize=True) for text in original_data],\n",
        "    \"Stemming\": [preprocess_text(text, stem=True) for text in original_data],\n",
        "    \"Remove Repeated Chars\": [preprocess_text(text, repeated_chars=True) for text in original_data],\n",
        "    \"Remove Short Words\": [preprocess_text(text, short_words=True) for text in original_data]\n",
        "}\n",
        "\n",
        "# Display comparison of techniques\n",
        "for technique, processed_data in techniques.items():\n",
        "    print(f\"\\n--- {technique} ---\")\n",
        "    for i, text in enumerate(processed_data):\n",
        "        print(f\"Text {i+1}: {text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fEgJJLBVGIcA",
        "outputId": "c599b55a-5cc3-479f-efdd-fcbaf7c87be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Original Data ---\n",
            "Text 1: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
            "\n",
            "Text 2: the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
            "\n",
            "Text 3: effective but too-tepid biopic\n",
            "\n",
            "Text 4: if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n",
            "\n",
            "Text 5: emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n",
            "\n",
            "\n",
            "--- Lowercase Only ---\n",
            "Text 1: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
            "Text 2: the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
            "Text 3: effective but too-tepid biopic\n",
            "Text 4: if you sometimes like to go to the movies to have fun , wasabi is a good place to start .\n",
            "Text 5: emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\n",
            "\n",
            "--- Remove Special Chars ---\n",
            "Text 1: the rock is destined to be the st centurys new conan and that hes going to make a splash even greater than arnold schwarzenegger jeanclaud van damme or steven segal\n",
            "Text 2: the gorgeously elaborate continuation of the lord of the rings trilogy is so huge that a column of words cannot adequately describe cowriterdirector peter jacksons expanded vision of j r r tolkiens middleearth\n",
            "Text 3: effective but tootepid biopic\n",
            "Text 4: if you sometimes like to go to the movies to have fun wasabi is a good place to start\n",
            "Text 5: emerges as something rare an issue movie thats so honest and keenly observed that it doesnt feel like one\n",
            "\n",
            "--- Remove Stop Words ---\n",
            "Text 1: rock destined st centurys new conan hes going make splash even greater arnold schwarzenegger jeanclaud van damme steven segal\n",
            "Text 2: gorgeously elaborate continuation lord rings trilogy huge column words adequately describe cowriterdirector peter jacksons expanded vision j r r tolkiens middleearth\n",
            "Text 3: effective tootepid biopic\n",
            "Text 4: sometimes like go movies fun wasabi good place start\n",
            "Text 5: emerges something rare issue movie thats honest keenly observed doesnt feel like one\n",
            "\n",
            "--- Lemmatization ---\n",
            "Text 1: rock destined st century new conan he going make splash even greater arnold schwarzenegger jeanclaud van damme steven segal\n",
            "Text 2: gorgeously elaborate continuation lord ring trilogy huge column word adequately describe cowriterdirector peter jackson expanded vision j r r tolkien middleearth\n",
            "Text 3: effective tootepid biopic\n",
            "Text 4: sometimes like go movie fun wasabi good place start\n",
            "Text 5: emerges something rare issue movie thats honest keenly observed doesnt feel like one\n",
            "\n",
            "--- Stemming ---\n",
            "Text 1: rock destin st centuri new conan he go make splash even greater arnold schwarzenegg jeanclaud van damm steven segal\n",
            "Text 2: gorgeous elabor continu lord ring trilog huge column word adequ describ cowriterdirector peter jackson expand vision j r r tolkien middleearth\n",
            "Text 3: effect tootepid biopic\n",
            "Text 4: sometim like go movi fun wasabi good place start\n",
            "Text 5: emerg someth rare issu movi that honest keenli observ doesnt feel like one\n",
            "\n",
            "--- Remove Repeated Chars ---\n",
            "Text 1: rock destined st centurys new conan hes going make splash even greater arnold schwarzenegger jeanclaud van damme steven segal\n",
            "Text 2: gorgeously elaborate continuation lord rings trilogy huge column words adequately describe cowriterdirector peter jacksons expanded vision j r r tolkiens middleearth\n",
            "Text 3: effective tootepid biopic\n",
            "Text 4: sometimes like go movies fun wasabi good place start\n",
            "Text 5: emerges something rare issue movie thats honest keenly observed doesnt feel like one\n",
            "\n",
            "--- Remove Short Words ---\n",
            "Text 1: rock destined centurys new conan hes going make splash even greater arnold schwarzenegger jeanclaud van damme steven segal\n",
            "Text 2: gorgeously elaborate continuation lord rings trilogy huge column words adequately describe cowriterdirector peter jacksons expanded vision tolkiens middleearth\n",
            "Text 3: effective tootepid biopic\n",
            "Text 4: sometimes like movies fun wasabi good place start\n",
            "Text 5: emerges something rare issue movie thats honest keenly observed doesnt feel like one\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required nltk data\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize lemmatizer and stop words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "default_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "# Define a custom stop word list: Remove common words but keep negation words\n",
        "custom_stopwords = default_stopwords - {'not', 'no', 'nor'}\n",
        "\n",
        "# Function to lowercase the text\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Function to remove special characters and punctuation\n",
        "def remove_special_chars(text):\n",
        "    return re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "\n",
        "# Function to remove stop words, but keep important ones like \"not\"\n",
        "def remove_stopwords(text):\n",
        "    words = word_tokenize(text)\n",
        "    return \" \".join([word for word in words if word not in custom_stopwords])\n",
        "\n",
        "# Function to apply lemmatization\n",
        "def lemmatize_text(text):\n",
        "    words = word_tokenize(text)\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "# Final preprocessing pipeline\n",
        "def preprocess_text(text):\n",
        "    text = to_lowercase(text)\n",
        "    text = remove_special_chars(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = lemmatize_text(text)\n",
        "    return text\n",
        "\n",
        "# Preprocess each text in the dataset\n",
        "preprocessed_data = [preprocess_text(text) for text in original_data]\n",
        "\n",
        "# Print results\n",
        "for i, text in enumerate(preprocessed_data):\n",
        "    print(f\"Text {i+1}: {text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xyptuyFYHuh1",
        "outputId": "3f987603-38cd-4f55-8768-6da3130fce31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1: rock destined st century new conan he going make splash even greater arnold schwarzenegger jeanclaud van damme steven segal\n",
            "Text 2: gorgeously elaborate continuation lord ring trilogy huge column word not adequately describe cowriterdirector peter jackson expanded vision j r r tolkien middleearth\n",
            "Text 3: effective tootepid biopic\n",
            "Text 4: sometimes like go movie fun wasabi good place start\n",
            "Text 5: emerges something rare issue movie thats honest keenly observed doesnt feel like one\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}